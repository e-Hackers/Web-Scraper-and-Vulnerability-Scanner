#!/bin/python3

from bs4 import BeautifulSoup
import requests
import urllib.parse
from collections import deque
import re
import threading
import urllib.robotparser
import sys

# Function to extract emails from a webpage
def extract_emails(url, scrapped_urls, urls):  # Pass scrapped_urls and urls as arguments
    new_emails = set()
    email_paths = {}  # Store the URL path for each email found
    try:
        response = requests.get(url, timeout=5, headers={'User-Agent': 'Mozilla/5.0'})
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        for anchor in soup.find_all("a", href=True):
            link = urllib.parse.urljoin(url, anchor['href'])
            if link not in scrapped_urls:
                scrapped_urls.add(link)
                urls.append(link)  # Add newly discovered URL to the deque
        emails = re.findall(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+', response.text)
        for email in emails:
            new_emails.add(email)
            email_paths[email] = url  # Store the URL path for each email found
    except requests.exceptions.RequestException:
        pass
    return new_emails, email_paths

# Function to search for sensitive data such as usernames and passwords
def search_sensitive_data(url):
    sensitive_data = set()
    sensitive_paths = {}  # Store the URL path for each sensitive data found
    try:
        response = requests.get(url, timeout=5, headers={'User-Agent': 'Mozilla/5.0'})
        response.raise_for_status()
        text = response.text.lower()
        if 'username' in text or 'user' in text:
            usernames = re.findall(r'username[:=]\s*(\S+)', text)
            for username in usernames:
                sensitive_data.add(username)
                sensitive_paths[username] = url  # Store the URL path for each username found
        if 'password' in text or 'pass' in text:
            passwords = re.findall(r'password[:=]\s*(\S+)', text)
            for password in passwords:
                sensitive_data.add(password)
                sensitive_paths[password] = url  # Store the URL path for each password found
        
        # Fetch and search in other file types
        file_extensions = ['js', 'php', 'html', 'txt', 'config', 'log']
        for ext in file_extensions:
            file_url = f"{url}.{ext}"
            file_response = requests.get(file_url, timeout=5, headers={'User-Agent': 'Mozilla/5.0'})
            if file_response.status_code == 200:
                sensitive_data.update(re.findall(r'some_pattern_to_search_in_files', file_response.text))
                sensitive_paths[file_url] = url  # Store the URL path for each sensitive data found in files
    except requests.exceptions.RequestException:
        pass
    return sensitive_data, sensitive_paths

# Function to check robots.txt rules
def is_allowed_by_robots(url):
    parsed_url = urllib.parse.urlparse(url)
    if parsed_url.scheme not in ['http', 'https']:  # Ignore unsupported URL schemes
        return False
    robots_url = f"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt"
    rp = urllib.robotparser.RobotFileParser()
    rp.set_url(robots_url)
    try:
        rp.read()
        return rp.can_fetch("*", url)
    except urllib.error.URLError:
        return False

# Main function
def main():
    user_url = input('[+] Enter Your Target to scan:')
    urls = deque([user_url])
    scrapped_urls = set()  # Store visited URLs
    emails = set()
    sensitive_data = set()
    email_paths = {}
    sensitive_paths = {}
    count = 0

    while urls and count < 100:
        count += 1
        url = urls.popleft()
        print(f'[{count}] Processing {url}')

        # Check if URL is allowed by robots.txt
        if is_allowed_by_robots(url):
            # Extract emails from the current URL
            new_emails, new_email_paths = extract_emails(url, scrapped_urls, urls)  # Pass scrapped_urls and urls
            emails.update(new_emails)
            email_paths.update(new_email_paths)
            
            # Search for sensitive data
            new_sensitive_data, new_sensitive_paths = search_sensitive_data(url)
            sensitive_data.update(new_sensitive_data)
            sensitive_paths.update(new_sensitive_paths)
        else:
            print(f"[-] Skipping {url} due to robots.txt restrictions")

    print('\n[+] Found Emails:')
    for email in emails:
        print(f"{email}: {email_paths[email]}")  # Print the email and its corresponding URL path

    print('\n[+] Found Sensitive Data:')
    for data in sensitive_data:
        print(f"{data}: {sensitive_paths[data]}")  # Print the sensitive data and its corresponding URL path

if __name__ == "__main__":
    main()

